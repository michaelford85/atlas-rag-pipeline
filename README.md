# ðŸ§© Atlas RAG Pipeline â€” Jupyter & Colab Edition

![diagram](docs/atlas_rag_pipeline.png)

This project demonstrates an end-to-end **Retrieval-Augmented Generation (RAG)** workflow combining:

- ðŸ§¬ **VoyageAI Embeddings** â€” to semantically encode text  
- ðŸŒ **MongoDB Atlas Vector Search** â€” to retrieve contextually similar content  
- ðŸ¤– **Ollama + Mistral** â€” to generate grounded, private LLM responses locally  

All code is runnable directly inside a **Jupyter Notebook or Google Colab**, with narrative explanations at each step to help users understand the full RAG process.

---

## ðŸš€ Overview

The notebook walks through each major stage of the RAG workflow:

| Step | Description |
|------|--------------|
| 1ï¸âƒ£ | Clone and set up the repository environment |
| 2ï¸âƒ£ | Mount Google Drive and securely load `.env.vault` keys |
| 3ï¸âƒ£ | Configure and test MongoDB Atlas + VoyageAI connections |
| 4ï¸âƒ£ | Build or update vector embeddings and indexes |
| 5ï¸âƒ£ | Install and configure Ollama with Mistral |
| 6ï¸âƒ£ | Verify model connectivity via the Ollama API |
| 7ï¸âƒ£ | Run full RAG queries combining retrieval and generation |
| 8ï¸âƒ£ | Optionally restart Ollama to refresh connections |

---

## ðŸ§­ System Flow Diagram

```mermaid
flowchart TD
    A[ðŸ”‘ Mount Google Drive / Load .env.vault] --> B[âš™ï¸ Clone and Setup Repository]
    B --> C[ðŸ§® Generate VoyageAI Embeddings]
    C --> D[ðŸŒ MongoDB Atlas Vector Search]
    D --> E[ðŸ“„ Retrieve Similar Documents]
    E --> F[ðŸ§± Assemble Prompt with Context]
    F --> G[ðŸ¤– Ollama (Mistral Model)]
    G --> H[ðŸ’¬ Generated Response to User]
    H --> I[ðŸ”„ (Optional) Restart Ollama Server]
```

---

## âš™ï¸ Prerequisites

1. **Install Ollama (locally or in Colab)**
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ollama pull mistral
   ```

2. **Set up MongoDB Atlas**
   - Create a collection and enable **Vector Search**  
   - Create a vector index (e.g., `fullplot_vector_index`)  
   - Populate it with text embeddings using VoyageAI  

3. **Obtain API Keys**
   - [VoyageAI API key](https://voyageai.com)  
   - [MongoDB Atlas connection string](https://cloud.mongodb.com)

4. **Create your `.env.vault`**
   ```bash
   MONGODB_URI="your_mongodb_connection_uri"
   VOYAGE_API_KEY="your_voyage_api_key"
   DB_NAME="sample_mflix"
   COLL_NAME="movies"
   FULLPLOT_INDEX_NAME="fullplot_vector_index"
   VECTOR_FIELD="fullplot_embedding"
   LLM_MODEL="mistral"
   ```

---

## ðŸ§© Notebook Storyline

Below are the **key narrative steps** used throughout the notebook â€” each represented by a Markdown cell above the matching code cell.

### 1ï¸âƒ£ Clone and Setup Repository
> Installs `git`, removes any old version, and clones the **atlas-rag-pipeline** repository.  
> Ensures the environment is current before running the pipeline.

### 2ï¸âƒ£ Enable Output Wrapping
> Adds CSS to wrap long text outputs in Colab for easier readability of model responses.

### 3ï¸âƒ£ Load Environment Variables
> Securely loads your `.env.vault` or `.env` file to configure credentials for MongoDB, VoyageAI, and Ollama.

### 4ï¸âƒ£ Mount Google Drive (Colab Only)
> Mounts Google Drive, retrieves your `atlas_rag_pipeline_dotenv_key.txt`, and sets it as the `DOTENV_KEY` variable.

### 5ï¸âƒ£ Build Vector Embeddings and Indexes
> Executes helper scripts to generate embeddings using VoyageAI and manage the MongoDB Atlas vector index.

### 6ï¸âƒ£ Set Up Ollama with Ansible
> Automatically installs and configures Ollama on Ubuntu, enabling consistent and repeatable setup.

### 7ï¸âƒ£ Test Mistral Model
> Uses a sample `curl` command to test the Ollama REST API and confirm that the local model responds correctly.

### 8ï¸âƒ£ Run Full RAG Query
> Executes `rag_mistral_complete_input.py` to run the retrieval-augmented generation process end-to-end.  
> Retrieves top-k documents, constructs a contextual prompt, and generates an answer using Mistral.

### 9ï¸âƒ£ Restart Ollama (Optional)
> Safely restarts the Ollama server to clear connections or memory before running new sessions.

---

## ðŸ§  Example Usage

### **Run a Full Query**
```bash
!python3 atlas-rag-pipeline/rag_mistral_complete_input.py "Which movies feature artificial intelligence or sentient robots?"
```

### **Expected Output**
```
ðŸ”Ž Generating embeddings for query...
ðŸ§  Retrieved documents:
- Bicentennial Man
- I, Robot
ðŸ’¬ Generated Answer:
These films explore human-like AI behavior and the tension between sentience and control.
```

---

## ðŸ§© Optional Maintenance Commands

### **Restart Ollama**
```bash
!pkill ollama || true
!nohup ollama serve > /tmp/ollama.log 2>&1 &
!sleep 10
```

> Use this if you encounter connection errors or need to reset the Ollama service during testing.

---

## ðŸ’¡ Tips

- When using Colab, rerun the **Ollama startup cell** after reconnecting to your runtime.  
- Keep your `atlas_rag_pipeline_dotenv_key.txt` in a private Google Drive folder for secure access.  
- You can modify the RAG query string to explore different movie-related or conceptual questions.

---

## ðŸ“œ License

MIT Â© 2025 Michael Ford
