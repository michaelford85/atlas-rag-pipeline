---
- name: Setup headless LLM server on Ubuntu 22.04 LTS
  hosts: localhost
  gather_facts: false
  become: true
  vars:
    ollama_model: "mistral"

  tasks:
    - name: Update apt cache
      ansible.builtin.apt:
        update_cache: true

    - name: Install prerequisites
      ansible.builtin.apt:
        name:
          - curl
          - git
        state: present

    - name: Install Ollama
      ansible.builtin.shell: |
        curl -fsSL https://ollama.com/install.sh | sh
      args:
        creates: /usr/local/bin/ollama

    - name: Detect if systemd is available
      ansible.builtin.stat:
        path: /run/systemd/system
      register: systemd_present

    - name: Start Ollama via systemd (if available)
      ansible.builtin.systemd:
        name: ollama
        enabled: true
        state: started
      when: systemd_present.stat.exists
      ignore_errors: true

    - name: Start Ollama manually if systemd not available
      ansible.builtin.shell: |
        nohup ollama serve > /var/log/ollama.log 2>&1 &
      when: not systemd_present.stat.exists

    - name: Wait for Ollama API to become available
      ansible.builtin.uri:
        url: http://localhost:11434/api/tags
        method: GET
        status_code: 200
      register: result
      retries: 10
      delay: 5
      until: result.status == 200
      ignore_errors: true

    - name: Pull the default model
      ansible.builtin.shell: "ollama pull {{ ollama_model }}"
      ignore_errors: true

    - name: Verify Ollama is listening
      ansible.builtin.shell: "ss -tulpen | grep 11434 || true"
      register: ss_output

    - name: Display listening ports
      ansible.builtin.debug:
        msg: "Ollama is listening on: {{ ss_output.stdout }}"